#Introduction

So, I've wanted to learn Machine Learning in a proper, systematic manner for a very long time. That's why I'm doing this.
As the Latin proverb _docendo discimus_ goes, "By teaching we learn". I'm trying to understand each ML model, one at a time, and their differences.

I'll be trying and showcasing many supervised Machine Learning classification models work.
I'll be using artifically-created training and test datasets that are always in two dimensions.
I'll be pictographically representing each of these models, and explaining exactly how to get a 'perfect score' on the test dataset.
I'll give perfectly clean datasets with no outliers.

This approach to introduce these algorithms has many drawbacks, the least of which is that it doesn't remotely resemble real Data Science.
However, my objective in dealing with each algorithm in this manner is many-fold.

The main objective of this approach is:

* to understand how each of the machine learning models work 'under the hood',
* to understand what sort of classification boundaries are formed by the algorithms,
* to implement these algorithms and be sure the implementation is correct

##What will be covered

The supervised machine learning algorithms I'll be covering in this way are as follows:

1. Perceptron
1. Decision Trees (CART algorithm)
1. Support Vector Machines (Linear)
1. k-Nearest Neighbours
1. Naive Bayes
1. Support Vector Machines (Kernels)
1. Neural Networks

I'll also try to give a real-world example where each (well, most) of these algorithms can be used,
but that'll be a lower priority than the artificial examples at the moment, so they'll come up later.
